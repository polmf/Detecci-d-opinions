{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews as mr\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('movie_reviews')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualització dels textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = mr.fileids('pos')\n",
    "\n",
    "# Imprimir el texto de los archivos\n",
    "for file_id in file_ids:\n",
    "    print(\"Texto del archivo:\", file_id)\n",
    "    print(mr.raw(file_id))\n",
    "    print(\"\\n-----------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr.words('pos/cv000_29590.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train i test\n",
    "train_pos = []\n",
    "test_pos = []\n",
    "\n",
    "i = 0\n",
    "while i < 700:\n",
    "    train_pos.append(nltk.pos_tag(mr.words(file_ids[i])))\n",
    "    i += 1\n",
    "    \n",
    "while i < 999:\n",
    "    test_pos.append(nltk.pos_tag(mr.words(file_ids[i])))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminem els signes de puntuació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "new_train = []\n",
    "\n",
    "for phrase in train_pos:\n",
    "    afegim = []\n",
    "    for word in phrase:\n",
    "        if not word[0].isdigit() and remove_punctuation(word[0]) == word[0]: # Verificar si la palabra no contiene ningún signo de puntuación\n",
    "            afegim.append(word)\n",
    "        #else:\n",
    "            #print('eliminem: ', word)\n",
    "    new_train.append(afegim)\n",
    "# Sobrescribir la lista original con la nueva lista de palabras procesadas\n",
    "#tagger = new_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = []\n",
    "\n",
    "for phrase in test_pos:\n",
    "    afegim = []\n",
    "    for word in phrase:\n",
    "        if not word[0].isdigit() and remove_punctuation(word[0]) == word[0]: # Verificar si la palabra no contiene ningún signo de puntuación\n",
    "            afegim.append(word)\n",
    "    \n",
    "    new_test.append(afegim)\n",
    "# Sobrescribir la lista original con la nueva lista de palabras procesadas\n",
    "#test = new_tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematització"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(p):\n",
    "  d = {'NN': 'n', 'NNS': 'n', 'NNP': 'n', 'NNPS': 'n',\n",
    "       'JJ': 'a', 'JJR': 'a', 'JJS': 'a', \n",
    "       'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v', \n",
    "       'RB': 'r', 'RBR': 'r', 'RBS': 'r'}\n",
    "  if p[1] in d:\n",
    "    return wnl.lemmatize(p[0], pos=d[p[1]])\n",
    "  return p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tagger = []\n",
    "\n",
    "for i, word in enumerate(tagger):\n",
    "    print('antes: ', word)\n",
    "    new_tagger.append((lemmatize(word), word[1]))\n",
    "    print('despues:', new_tagger[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\nltk\\tag\\hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\nltk\\tag\\hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n",
      "c:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11829235367395949"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
    "HMM = trainer.train_supervised(new_train)\n",
    "\n",
    "HMM.accuracy(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afegim funció de smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8944302082691916"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LID(fd, bins):\n",
    "  return nltk.probability.LidstoneProbDist(fd, 0.1, bins)\n",
    "trainer = nltk.tag.hmm.HiddenMarkovModelTrainer()\n",
    "\n",
    "HMM = trainer.train_supervised(new_train, estimator=LID)\n",
    "HMM.accuracy(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8944302082691916"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HMM = nltk.HiddenMarkovModelTagger.train(new_train)\n",
    "HMM.accuracy(new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pràctica 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dos primers fitxers:  ['pos/cv000_29590.txt', 'pos/cv001_18431.txt']\n",
      "Número de textos amb connotació negativa:  1000\n",
      "Head de paraules en el primer text amb connotació positiva:  ['films', 'adapted', 'from', 'comic', 'books', 'have', ...]\n"
     ]
    }
   ],
   "source": [
    "print('Dos primers fitxers: ', mr.fileids('pos')[:2])\n",
    "print('Número de textos amb connotació negativa: ', len(mr.fileids('neg')))\n",
    "print('Head de paraules en el primer text amb connotació positiva: ', mr.words(mr.fileids('pos')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes: films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before .\n",
      "yes: for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen .\n",
      "yes: to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd .\n",
      "yes: the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes .\n",
      "yes: in other words , don't dismiss this film because of its source .\n",
      "yes: if you can get past the whole comic book thing , you might find another stumbling block in from hell's directors , albert and allen hughes .\n",
      "yes: getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that's set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ?\n",
      "yes: the ghetto in question is , of course , whitechapel in 1888 london's east end .\n",
      "yes: it's a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision .\n",
      "yes: when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case .\n",
      "yes: abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium .\n",
      "yes: upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn't so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can't stomach .\n",
      "yes: i don't think anyone needs to be briefed on jack the ripper , so i won't go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay .\n",
      "yes: in the comic , they don't bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end .\n",
      "yes: it's funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts .\n",
      "yes: and from hell's ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car/who made steve guttenberg a star ? \" ) .\n",
      "yes: don't worry - it'll all make sense when you see it .\n",
      "yes: now onto from hell's appearance : it's certainly dark and bleak enough , and it's surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) .\n",
      "yes: the print i saw wasn't completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don't say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic .\n",
      "yes: oscar winner martin childs' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place .\n",
      "yes: even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent .\n",
      "yes: ians holm ( joe gould's secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham .\n",
      "yes: i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn't half bad .\n",
      "yes: the film , however , is all good .\n",
      "yes: 2 : 00 - r for strong violence/gore , sexuality , language and drug content\n",
      "yes: \n"
     ]
    }
   ],
   "source": [
    "text = mr.raw(mr.fileids('pos')[0])\n",
    "\n",
    "frases = re.split(r'[\\n]', text)\n",
    "\n",
    "# Imprimir les frases\n",
    "for frase in frases:\n",
    "    print('yes:', frase.strip())  # Utilitzem strip() per eliminar els espais en blanc addicionals al principi i al final de cada frase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "# Afegim només les paraules de cada text en una nova llista pels textos amb connotació positiva\n",
    "positius = []\n",
    "for i in range(len(mr.fileids('pos'))):\n",
    "    afegim = []\n",
    "    for word in mr.words(mr.fileids('pos')[i]):\n",
    "        if not word.isdigit() and remove_punctuation(word) == word: # Verificar si la palabra no contiene ningún signo de puntuación o dígito\n",
    "            afegim.append(word)\n",
    "            \n",
    "    positius.append(afegim)\n",
    "    \n",
    "    \n",
    "# Afegim només les paraules de cada text en una nova llista pels textos amb connotació negativa\n",
    "negatius = []\n",
    "for i in range(len(mr.fileids('neg'))):\n",
    "    afegim = []\n",
    "    for word in mr.words(mr.fileids('neg')[i]):\n",
    "        if not word.isdigit() and remove_punctuation(word) == word: # Verificar si la palabra no contiene ningún signo de puntuación o dígito\n",
    "            afegim.append(word)\n",
    "            \n",
    "    negatius.append(afegim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El bo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenir les opinions del corpus de Movie Reviews\n",
    "documents = [(mr.raw(fileid), category)\n",
    "             for category in mr.categories()\n",
    "             for fileid in mr.fileids(category)]\n",
    "\n",
    "etiquetes = [opinion[1] for opinion in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "textos = []\n",
    "afegim = []\n",
    "for opinio in documents:\n",
    "    # Separa les paraules del text sense tenir en compte els salts de línia\n",
    "    words = re.split(r'\\s+', opinio[0])\n",
    "    for word in words:\n",
    "        if not word.isdigit() and remove_punctuation(word) == word: # Verificar si la palabra no contiene ningún signo de puntuación o dígito\n",
    "            afegim.append(word)\n",
    "\n",
    "    textos.append(' '.join(afegim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(textos, etiquetes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorització dels textos\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "for i in X.toarray():\n",
    "    for index, j in enumerate(i):\n",
    "        count[index] = count.get(index, 0) + j\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "index_eliminar = {index: freq for index, freq in count.items() if freq < 10}\n",
    "\n",
    "matriu = np.delete(X, index_eliminar, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Genera 100 índices aleatorios únicos dentro del rango de 0 a 2000\n",
    "indices_train = np.random.choice(range(2000), size=150, replace=False)\n",
    "\n",
    "train_textos = [textos[i] for i in indices_train]\n",
    "train_etiquetes = [etiquetes[i] for i in indices_train]\n",
    "\n",
    "\"\"\"\n",
    "np.random.seed(32)\n",
    "\n",
    "indices_test = np.random.choice(range(2000), size=20, replace=False)\n",
    "\n",
    "test_textos = [textos[i] for i in indices_test]\n",
    "test_etiquetes = [etiquetes[i] for i in indices_test]\"\"\"\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_textos, train_etiquetes, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Vectorització dels textos\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "\n",
    "#print(\"---\"*20)\n",
    "#print(\"Visualitzacio de vectorizer: \", vectorizer.get_feature_names_out())\n",
    "#print('\\n')\n",
    "#print(\"---\"*20)\n",
    "\n",
    "# Entrenament del model Naive Bayes amb validació creuada (cross-validation)\n",
    "naive_bayes_model = MultinomialNB()\n",
    "nb_scores = cross_val_score(naive_bayes_model, X, y_train, cv=5)\n",
    "naive_bayes_model = naive_bayes_model.fit(X, y_train)\n",
    "\n",
    "# Entrenament del model SVM amb validació creuada (cross-validation)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_scores = cross_val_score(svm_model, X, y_train, cv=5)\n",
    "svm_model = svm_model.fit(X, y_train)\n",
    "\n",
    "# Resultats\n",
    "print(\"Precisió del model Naive Bayes amb validació creuada:\", nb_scores.mean())\n",
    "print(\"Precisió del model SVM amb validació creuada:\", svm_scores.mean())\n",
    "\n",
    "print(\"---\"*20)\n",
    "\n",
    "\n",
    "nb_predict = naive_bayes_model.predict(X_test)\n",
    "precisio_nb = accuracy_score(y_test, nb_predict)\n",
    "print(\"Precisió del model Naive Bayes:\", precisio_nb)\n",
    "\n",
    "svm_predict = svm_model.predict(X_test)\n",
    "precisio_svm = accuracy_score(y_test, svm_predict)\n",
    "print(\"Precisió del model SVM:\", precisio_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Desactivar todos los warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (accuracy) media con Naive Bayes: 0.8125\n",
      "Precisión (accuracy) con Naive Bayes: 0.82\n",
      "Matriz de confusión con Naive Bayes:\n",
      "[[162  36]\n",
      " [ 36 166]]\n",
      "Precisión (accuracy) media con Logistic Regression: 0.835\n",
      "Precisión (accuracy) con Logistic Regression: 0.835\n",
      "Matriz de confusión con Logistic Regression:\n",
      "[[160  38]\n",
      " [ 28 174]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Obtener las revisiones y sus categorías (pos/neg)\n",
    "documents = [(list(mr.words(fileid)), category)\n",
    "             for category in mr.categories()\n",
    "             for fileid in mr.fileids(category)]\n",
    "\n",
    "# Mezclar los documentos para garantizar una distribución aleatoria\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Preprocesamiento: unir las palabras y eliminar stopwords y signos de puntuación\n",
    "#stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "preprocessed_documents = []\n",
    "for words, category in documents:\n",
    "    words = [word.lower() for word in words if word.lower() not in punctuation]\n",
    "    preprocessed_documents.append((' '.join(words), category))\n",
    "\n",
    "# Separar los datos en características (X) y etiquetas (y)\n",
    "X = [text for text, category in preprocessed_documents]\n",
    "y = [category for text, category in preprocessed_documents]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear un pipeline con CountVectorizer y un clasificador\n",
    "pipeline_NB = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_LR = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Validación cruzada para obtener la precisión media del modelo NB\n",
    "scores_NB = cross_val_score(pipeline_NB, X_train, y_train, cv=5)\n",
    "print(\"Precisión (accuracy) media con Naive Bayes:\", scores_NB.mean())\n",
    "\n",
    "# Entrenar y evaluar el modelo con Naive Bayes\n",
    "pipeline_NB.fit(X_train, y_train)\n",
    "y_pred_NB = pipeline_NB.predict(X_test)\n",
    "print(\"Precisión (accuracy) con Naive Bayes:\", accuracy_score(y_test, y_pred_NB))\n",
    "print(\"Matriz de confusión con Naive Bayes:\")\n",
    "print(confusion_matrix(y_test, y_pred_NB))\n",
    "\n",
    "# Validación cruzada para obtener la precisión media del modelo LR\n",
    "scores_LR = cross_val_score(pipeline_LR, X_train, y_train, cv=5)\n",
    "print(\"Precisión (accuracy) media con Logistic Regression:\", scores_LR.mean())\n",
    "\n",
    "# Entrenar y evaluar el modelo con Logistic Regression\n",
    "pipeline_LR.fit(X_train, y_train)\n",
    "y_pred_LR = pipeline_LR.predict(X_test)\n",
    "print(\"Precisión (accuracy) con Logistic Regression:\", accuracy_score(y_test, y_pred_LR))\n",
    "print(\"Matriz de confusión con Logistic Regression:\")\n",
    "print(confusion_matrix(y_test, y_pred_LR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho tornem a fer tot pero ara amb count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_id in file_ids:\n",
    "    print(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StreamBackedCorpusView' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1276\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1277\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\.pyenv\\pyenv-win\\versions\\3.11.0\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StreamBackedCorpusView' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(mr.words(file_id) for file_id in file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cv000_29590', 'cv001_18431', 'cv002_15918', ..., 'cv999_13106',\n",
       "       'pos', 'txt'], dtype=object)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
